{
  
    
        "post0": {
            "title": "Vector Semantics and Embeddings",
            "content": "Distributional Hypothesis . åˆ†å¸ƒå‡è®¾ã€‚å•è¯åˆ†å¸ƒçš„ç›¸ä¼¼åº¦å’Œå•è¯æ„ä¹‰çš„ç›¸ä¼¼åº¦çš„å…³ç³»ã€‚ . Lexical Semantics . è¯æ±‡è¯­ä¹‰ã€‚è¯­è¨€å­¦ä¸­ä»å¤šä¸ªæ–¹é¢è€ƒå¯Ÿå•è¯çš„è¯­ä¹‰ã€‚ . Lemmas and Senses . åœ¨å½¢æ€å­¦å’Œè¯å…¸ç¼–çº‚ä¸­ï¼Œå¼•ç†æ˜¯ä¸€ç»„å•è¯çš„è§„èŒƒå½¢å¼ï¼Œè¯å…¸å½¢å¼æˆ–å¼•ç”¨å½¢å¼ã€‚ . å¯¹äºæ¯ä¸€ç§å¼•ç”¨å½¢å¼ï¼Œéƒ½å¯ä»¥æœ‰å¤šç§è¯­ä¹‰ï¼Œç§°ä¸ºä¸€è¯å¤šä¹‰ï¼Œæ¯ä¸€ç§è¯­ä¹‰ç§°ä½œä¸€ä¸ª word scene ã€‚ . | Word Synonymy . åŒä¹‰ã€‚ä» scene çš„è§’åº¦æ¥è®²ï¼Œå¦‚æœä¸¤ä¸ªå•è¯çš„è¯­ä¹‰ä¹‹ä¸€æ˜¯ä¸€è‡´çš„ï¼Œé‚£ä¹ˆå¯ä»¥è¯´è¿™ä¸¤ä¸ªå•è¯æ˜¯åŒä¹‰çš„ã€‚ä» word çš„è§’åº¦çœ‹ï¼ŒåŒä¹‰çš„ä¸€ä¸ªæ›´æ­£å¼çš„å®šä¹‰å¯ä»¥æ˜¯ï¼šå¦‚æœä¸¤ä¸ªå•è¯å¯ä»¥åœ¨ä»»ä½•å¥å­ä¸­ç›¸äº’æ›¿æ¢è€Œä¸æ”¹å˜å¥å­çš„çœŸå®æ¡ä»¶ï¼Œåˆ™è¯¥ä¸¤ä¸ªå•è¯æ˜¯åŒä¹‰è¯ã€‚ . ä½†æ˜¯ï¼Œä¹Ÿè®¸æ²¡æœ‰çœŸæ­£çš„åŒä¹‰è¯ï¼Œå› ä¸ºè¯æ±‡çš„è¯­ä¹‰çš„å·®å¼‚æ€»æ˜¯å’Œåœ¨è¯­è¨€å­¦ä¸Šçš„å½¢æ€çš„å·®å¼‚ç›¸å…³è”ã€‚æ¯”å¦‚ï¼Œwater å’Œ $h_2o$ éƒ½æ˜¯æ°´çš„æ„æ€ï¼Œä½†æ˜¯ $h_2o$ ä¸€èˆ¬å‡ºç°åœ¨ç§‘å­¦ç›¸å…³çš„æ–‡çŒ®ï¼Œè€Œ water æ›´é€‚åˆäºå¹³å¸¸çš„æ–‡æœ¬ä¸­ï¼Œè¿™ç§ç±»å‹çš„åŒºåˆ«ä¹Ÿæ˜¯è¯­ä¹‰çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥è¯´åŒä¹‰çœŸå®çš„æ„æ€æ˜¯å¤§æ¦‚æ„æ€ä¸€æ ·ï¼Œå¹¶éå®Œå…¨ä¸€è‡´ã€‚æˆ–è®¸å¯ä»¥è¿™ä¹ˆæƒ³ï¼šåœ¨å·²çŸ¥å·²ç»å­˜åœ¨ä¸€ä¸ªè¯å¯ä»¥æè¿°æƒ³è¡¨è¾¾çš„æ„æ€ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿˜è¦åˆ›é€ ä¸€ä¸ªåŒæ ·è¯­ä¹‰çš„æ–°è¯ï¼ŸåŸå› å¯èƒ½æ˜¯æ—§è¯çš„è¯­ä¹‰å’Œæƒ³è¡¨è¾¾çš„è¯­ä¹‰æœ‰äº›è®¸å·®åˆ«ï¼Œéœ€è¦ä¸€ä¸ªæ–°è¯æ¥è¡¨ç¤ºã€‚ . | Word Similarity . ç›¸ä¼¼ã€‚ç›¸ä¼¼æ€§å…³æ³¨çš„æ˜¯å•è¯æœ¬èº«è€Œä¸æ˜¯å•è¯çš„è¯­ä¹‰ã€‚æ¯”å¦‚ dog å’Œ catã€‚ . | Word Relatedness . å…³è”æ€§ã€‚å…³è”æ€§ä¸è¦æ±‚ç›¸ä¼¼ï¼Œæ›´å…³æ³¨å•è¯ä¹‹é—´çš„å…³ç³»ã€è”ç³»ã€‚æ¯”å¦‚ water å’Œ cupï¼Œä¸¤è€…ä¸ç›¸ä¼¼ï¼Œä½†æ˜¯å’ŒåŒä¸€äº‹ä»¶ç›¸å…³â€“å–æ°´ã€‚ä¸€ç§è¾ƒä¸ºå¸¸è§çš„å…³ç³»æ˜¯ semantic field ã€‚semantic field æ˜¯æŒ‡ä¸€ç»„å•è¯ï¼Œè¦†ç›–ç‰¹å®šçš„è¯­ä¹‰åŸŸå¹¶ä¸”å½¼æ­¤ä¹‹é—´å…·æœ‰ç»“æ„åŒ–å…³ç³»ã€‚æ¯”å¦‚ï¼Œå¯¹äº hospital è¿™ä¸ª semantic fieldï¼Œå¯ä»¥åŒ…å« surgeon, scalpel, nurse, anesthetic, hospital è¿™äº›è¯ã€‚Semantic field ä¸ topic model ç›¸å…³ï¼ŒäºŒè€…éƒ½å¯ä»¥æ— ç›‘ç£çš„å­¦ä¹ æ–‡æœ¬ä¸­çš„ä¸»é¢˜ç»“æ„ï¼Œéå¸¸æœ‰ç”¨ã€‚ . è¿™ä¹ˆè¯´æ¥ï¼Œå¸¸è¯´çš„ä¸»é¢˜ä»¥åŠä¸»é¢˜ä¹‹é—´çš„å…³ç³»ï¼Œå…¶å®å°±æ˜¯å•è¯ä¹‹é—´çš„å…³è”æ€§å’Œç»“æ„ã€‚ . | Semantic Frames and Roles . Semantic frames å’Œ semantic field ç›¸ä¼¼ï¼ŒæŒ‡çš„æ˜¯è¡¨ç¤ºç‰¹å®šäº‹ä»¶çš„ä¸åŒè§‚ç‚¹å’Œå‚ä¸è€…çš„ä¸€ç»„å•è¯ã€‚æ¯”å¦‚åœ¨å•†ä¸šäº¤æ˜“ä¸­ï¼Œæœ‰ buyï¼Œsellï¼Œpay ç­‰åŠ¨ä½œï¼Œæ¯ä¸ªåŠ¨ä½œä¹Ÿä»£è¡¨ç€ä¸€ä¸ªè§’è‰²ï¼Œåˆ†åˆ«æ˜¯ buyerï¼Œsellerï¼Œmoneyã€‚ . | Connotation . è•´æ¶µæˆ–è€…æƒ…æ„Ÿæ„ä¹‰ã€‚è¤’ä¹‰ã€è´¬ä¹‰æˆ–è€…ç§¯æã€æ¶ˆæï¼Œåœ¨åŠ ä¸€ä¸ªä¸­ç«‹ã€‚æƒ…æ„Ÿåˆ†æä¹Ÿæ˜¯ nlp ä¸­ä¸€ä¸ªé‡è¦çš„æ–¹å‘ã€‚ . å¦‚ä½•è¡¡é‡ä¸€ä¸ªè¯æ‰€è•´æ¶µçš„æƒ…æ„Ÿï¼ŸOsgood ä»ä¸‰ä¸ªæ–¹é¢æ¥ç»™å•è¯æ‰“åˆ†ï¼š . valence: the pleasantness of the stimulusï¼Œåˆºæ¿€çš„æ„‰æ‚¦æ„Ÿ | arousal: the intensity of emotion provoked by the stimulusï¼Œè¢«åˆºæ¿€æ‰€å¼•èµ·çš„æ„Ÿæƒ…å¼ºåº¦ | dominance: the degree of control exerted by the stimulusï¼Œåˆºæ¿€æ‰€æ–½åŠ çš„æ§åˆ¶ç¨‹åº¦ | . é€šè¿‡å¯¹ä¸€ä¸ªå•è¯ä»ä»¥ä¸Šä¸‰ä¸ªæ–¹é¢çš„è¡¡é‡ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªä¸‰ç»´ç©ºé—´ä¸­çš„ç‚¹ï¼Œè¿™æ˜¯å†å²ä¸Šå…³äºå‘é‡è¯­ä¹‰çš„ç¬¬ä¸€æ¬¡è¡¨è¿°ï¼Œçªç ´æ€§çš„æƒ³æ³•ğŸ’¡ï¼ï¼ . | . Vector Semantics . å‘é‡è¯­ä¹‰ã€‚å®ä¾‹åŒ–åˆ†å¸ƒå‡è®¾ï¼Œå…·æœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„å…·æœ‰ç›¸ä¼¼çš„å«ä¹‰ï¼Œæ ¹æ®å•è¯åœ¨æ–‡æœ¬ä¸­çš„åˆ†å¸ƒå­¦ä¹ å•è¯çš„å«ä¹‰ã€‚å•è¯çš„å«ä¹‰ä¹Ÿç§°ä¸ºè¯åµŒå…¥ï¼Œå³ Embeddingï¼ŒåŸå› æ˜¯å°†å…¶åµŒå…¥åˆ°æŸä¸ªå‘é‡ç©ºé—´ä¸­ã€‚è¿™ç§è·å¾—å•è¯è¡¨ç¤ºçš„æ–¹æ³•æ˜¯å±äºè¡¨ç¤ºå­¦ä¹ ï¼ˆPresentation Learningï¼‰ï¼ŒåŒºåˆ«äºç‰¹å¾å·¥ç¨‹ä¸­äººå·¥åˆ›é€ çš„è¡¨ç¤ºã€‚ . Co-occurrence Matrix . åŸºæœ¬ä¸Šï¼Œå‘é‡çš„è·å¾—éƒ½åŸºäºå…±ç°çŸ©é˜µçš„æƒ³æ³•ï¼Œä¸»è¦æœ‰ä¸¤ç§çŸ©é˜µï¼šterm-document matrix å’Œ term-term matrixã€‚ . Documents as vectors . åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼Œè¦å¬å›æ–‡æ¡£ï¼Œé‚£ä¹ˆå¦‚ä½•è¡¨ç¤ºæ–‡æ¡£å‘¢ï¼Ÿæ–‡æ¡£ç”±å•è¯ç»„æˆï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ–‡æ¡£ä¸­å„ä¸ªå•è¯å‡ºç°çš„é¢‘æ¬¡æ¥è¡¨ç¤ºæ–‡æ¡£ï¼Œç›¸ä¼¼çš„æ–‡æ¡£æ‹¥æœ‰ç›¸ä¼¼çš„å•è¯åˆ†å¸ƒã€‚ . Â  doc_1 doc_2 doc_3 doc_4 . word_1 | 1 | 0 | 7 | 11 | . word_2 | 114 | 80 | 62 | 89 | . word_3 | 36 | 58 | 1 | 4 | . word_4 | 20 | 15 | 2 | 3 | . å¯¹äºæ¯ä¸€åˆ—ï¼Œå°±æ˜¯ç›¸åº”æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºï¼Œç»´åº¦æ˜¯ $ lvert V rvert$ ï¼Œè¯è¡¨çš„å¤§å°ï¼Œå¹¶ä¸”è¿™ä¸ªå‘é‡æ˜¯ç¨€ç–çš„ã€‚ . Words as vectors: document dimensions . åœ¨ä¸Šè¡¨ä¸­ï¼Œå¦‚æœæŒ‰è¡Œæ¥çœ‹ï¼Œæ¯ä¸€è¡Œéƒ½å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå•è¯çš„å‘é‡è¡¨ç¤ºï¼Œç»´åº¦æ˜¯ $ lvert D rvert$ï¼Œè¯­æ–™åº“æ–‡æ¡£çš„æ•°ç›®ï¼Œå¹¶ä¸”è¿™ä¸ªå‘é‡ä¹Ÿæ˜¯ç¨€ç–çš„ã€‚ç›¸ä¼¼çš„è¯ä¹Ÿå€¾å‘äºå‡ºç°åœ¨ç›¸ä¼¼çš„æ–‡æ¡£ä¸­ã€‚Term-document matrix æ—¢å¯ä»¥ç”¨æ¥è¡¨ç¤ºæ–‡æ¡£ä¹Ÿå¯ä»¥ç”¨æ¥è¡¨ç¤ºå•è¯ã€‚ . Words as vectors: word dimensions . å¯ä¸å¯ä»¥ç”¨è¯æ¥è¡¡é‡è¯å‘¢ï¼Ÿæ˜¾ç„¶å¯ä»¥ï¼Œè¿™å°±æ˜¯ Term-term matrix çš„æƒ³æ³•ï¼Œåœ¨ä¸€ä¸ªå°çª—å£å†…ï¼Œä¸¤ä¸ªè¯å…±ç°çš„é¢‘æ¬¡ã€‚ . Â  aardvark â€¦ computer data result pie sugar â€¦ . cherry | 0 | â€¦ | 2 | 8 | 9 | 442 | 25 | â€¦ | . strawberry | 0 | â€¦ | 0 | 0 | 1 | 60 | 19 | â€¦ | . digital | 0 | â€¦ | 1670 | 1683 | 85 | 5 | 4 | â€¦ | . information | 0 | â€¦ | 3325 | 3982 | 378 | 5 | 13 | â€¦ | . æ¯ä¸€ä¸ªå‘é‡çš„ç»´åº¦æ˜¯ $ lvert V rvert$ ï¼Œä¹Ÿæ˜¯ç¨€ç–ã€‚ . TF-IDF . å…±ç°çŸ©é˜µï¼Œä¸ç®¡æ˜¯ term-document è¿˜æ˜¯ term-term ï¼Œå…¶ä¸­æ¯ä¸€é¡¹éƒ½æ˜¯åŸå§‹é¢‘æ¬¡ã€‚åŸå§‹é¢‘æ¬¡å¹¶ä¸èƒ½å¾ˆå¥½åœ°è¡¡é‡è¯ä¹‹é—´çš„å…³ç³»ã€‚å¯¹äºä¸€ä¸ªè¯æ¥è¯´ï¼Œå¦‚æœæœ‰ä¸€ä¸ªè¯å’Œå®ƒçš„å…±ç°é¢‘ç‡å¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€èˆ¬æƒ…å†µä¸‹è¿™ä¸ªè¯å¯¹è¡¡é‡å®ƒæ˜¯èµ·é‡è¦ä½œç”¨çš„ï¼›ä½†æ˜¯ï¼Œå¦‚æœè¿™ä¸ªè¯è¿‡äºé¢‘ç¹çš„å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªè¯å¯¹äºè¡¡é‡å…¶å…±ç°çš„è¯å°±æ²¡é‚£ä¹ˆé‡è¦äº†ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æœ‰ä¸¤ä¸ªï¼šTF-IDF å’Œ PPMIã€‚ . TF-IDF åŸºäº term-document matrixã€‚ . Term Frequency . tft,d=log(count(t,d)+1)tf_{t,d}=log(count(t,d)+1)tft,dâ€‹=log(count(t,d)+1) | Inverse Doucument Frequency . idft=log(Ndft)idf_t=log( frac{N}{df_t})idftâ€‹=log(dftâ€‹Nâ€‹) | TF-IDF . wt,d=tft,dÃ—idftw_{t,d}=tf_{t,d} times idf_twt,dâ€‹=tft,dâ€‹Ã—idftâ€‹ | . Positive Pointwise Mutual Information (PPMI) . äº’ä¿¡æ¯è¡¡é‡ä¸¤ä¸ªå˜é‡é—´çš„ä¾èµ–ç¨‹åº¦ï¼Œæ•°å€¼ä¸Šæ˜¯ç‚¹é—´äº’ä¿¡æ¯çš„æœŸæœ›ã€‚ç›´è§‚ä¸Šï¼Œäº’ä¿¡æ¯åº¦é‡ X å’Œ Y å…±äº«çš„ä¿¡æ¯ï¼šå®ƒåº¦é‡çŸ¥é“è¿™ä¸¤ä¸ªå˜é‡å…¶ä¸­ä¸€ä¸ªï¼Œå¯¹å¦ä¸€ä¸ªä¸ç¡®å®šåº¦å‡å°‘çš„ç¨‹åº¦ã€‚åœ¨ NLP ä¸­ï¼Œå¯ä»¥ç”¨ç‚¹é—´äº’ä¿¡æ¯æ¥åº¦é‡ä¸¤ä¸ªè¯çš„ç›¸å…³ç¨‹åº¦ã€‚ . PPMI(w,c)=max(logP(w,c)P(w)P(c),0)PPMI(w,c)=max(log frac{P(w,c)}{P(w)P(c)},0)PPMI(w,c)=max(logP(w)P(c)P(w,c)â€‹,0) . ç½•è§è¯å€¾å‘äºé«˜PMIå€¼ã€‚ . ä¸€èˆ¬ä¼šç”¨ SVD è¿›è¡Œé™ç»´ã€‚ . Word2vec . è§ word2vec.md . Glove . è§ glove.md . Something About Embeddings . Visualizing . èšç±» | t-SNE | åˆ—å‡ºæœ€ç›¸ä¼¼çš„è¯ | . Semantics Properties . ç›¸ä¼¼ä¸ç›¸å…³ï¼šçª—å£å°è¶‹å‘ç›¸ä¼¼ï¼Œçª—å£å¤§è¶‹å‘ç›¸å…³ | ä¸€é˜¶å…±ç°ï¼šç›¸é‚»ï¼›äºŒé˜¶å…±ç°ï¼šç›¸é‚»ç›¸ä¼¼ | æ¨ç† | ä¸åŒæ—¶é—´æ®µçš„æ–‡æœ¬å¾—åˆ°çš„å‘é‡éšç€æ—¶é—´æ˜¯å˜åŒ–çš„ | . Bias . æ€§åˆ« | èŒä¸š | .",
            "url": "https://cbqin.github.io/jiangnandiqing/markdown/2021/02/20/vector-semantics-and-embeddings.html",
            "relUrl": "/markdown/2021/02/20/vector-semantics-and-embeddings.html",
            "date": " â€¢ Feb 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Word2vec",
            "content": "Word2vec åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å‹ï¼šCBOW å’Œ Skip-Gram ä»¥åŠä¸¤ä¸ªä¼˜åŒ–ç®—æ³•ï¼šHierarchical Softmax å’Œ Negative Samplingã€‚ . CBOW . ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­é—´è¯ã€‚ . . Notations . $w_i$: è¯è¡¨ $V$ ä¸­ç¬¬ $i$ ä¸ªå•è¯ã€‚ | $ mathcal{V} in R^{n times lvert V rvert}$: è¾“å…¥å•è¯çŸ©é˜µã€‚ | $ mathcal{v_i}$: $ mathcal{V}$ çš„ç¬¬ $i$ åˆ—ï¼Œ$w_i$ çš„è¾“å…¥å‘é‡è¡¨ç¤ºã€‚ | $ mathcal{U} in R^{ lvert V rvert times n}$: è¾“å‡ºå•è¯çŸ©é˜µã€‚ | $ mathcal{u_i}$: $ mathcal{U}$ çš„ç¬¬ $i$ è¡Œï¼Œ$w_i$ çš„è¾“å‡ºå‘é‡è¡¨ç¤ºã€‚ | . Steps . ä¸Šä¸‹æ–‡ç”¨ç‹¬çƒ­ç¼–ç è¡¨ç¤ºï¼Œçª—å£å¤§å°ä¸º $m$: $(x^{(c-m)},â€¦,x^{(c-1)},x^{(c+1)},â€¦,x^{(c+m)} in R^{ lvert V rvert})$ | è¾“å…¥å‘é‡è½¬æ¢: ($ mathcal{v_{c-m}}= mathcal{V}x^{(c-m)},â€¦, mathcal{v_{c-1}}= mathcal{V}x^{(c-1)}, mathcal{v_{c+1}}= mathcal{V}x^{(c+1)},â€¦, mathcal{v_{c+m}}= mathcal{V}x^{(c+m)}$) | å¹³å‡: $ hat{v}= frac{v_{c-m}+â€¦+v_{c-1}+v_{c+1}+â€¦+v_{c+m}}{2m} in{R^n}$ | å¾—åˆ†: $z= mathcal{U} hat{v} in{R^{ lvert V rvert}}$ | æ¦‚ç‡: $ hat{y}=softmax(z) in{R^{ lvert V rvert}}$ | æ¯”è¾ƒ: $ hat{y} in{R^{ lvert V rvert}}$ ä¸ $y in{R^{ lvert V rvert}}$ | Object Function . ä¸€èˆ¬é€‰æ‹©äº¤å‰ç†µæ¥è¡¡é‡åˆ†å¸ƒ $ hat{y}$ å’Œ $y$ çš„å·®å¼‚ã€‚ . H(y,y^)=âˆ’âˆ‘j=1âˆ£Vâˆ£yjlogâ¡y^=âˆ’yilogâ¡yi^=âˆ’logâ¡yi^ begin{aligned} H(y, hat{y}) &amp;= - sum_{j=1}^{ lvert V rvert}y_j log{ hat{y}} &amp;=-y_i log{ hat{y_i}} &amp;= - log{ hat{y_i}} end{aligned}H(y,y^â€‹)â€‹=âˆ’j=1âˆ‘âˆ£Vâˆ£â€‹yjâ€‹logy^â€‹=âˆ’yiâ€‹logyiâ€‹^â€‹=âˆ’logyiâ€‹^â€‹â€‹ . æ‰€ä»¥æœ‰ï¼š . minimizeÂ J=âˆ’logâ¡P(wcâˆ£wcâˆ’m,...,wcâˆ’1,wc+1,wc+m)=âˆ’logâ¡P(ucâˆ£v^)=âˆ’logâ¡exp(ucTv^)âˆ‘j=1âˆ£Vâˆ£exp(ujTv^)=âˆ’ucTv^+logâ¡âˆ‘j=1âˆ£Vâˆ£exp(ujTv^) begin{aligned} minimize J &amp;= - log{P(w_c|w_{c-m},...,w_{c-1},w_{c+1},w_{c+m})} &amp;= - log{P(u_c| hat{v})} &amp;= - log{ frac{exp(u_c^T hat{v})}{ sum_{j=1}^{ lvert V rvert}exp(u_j^T hat{v})}} &amp;= -u_c^T hat{v} + log{ sum_{j=1}^{ lvert V rvert}exp(u_j^T hat{v})} end{aligned}minimizeÂ Jâ€‹=âˆ’logP(wcâ€‹âˆ£wcâˆ’mâ€‹,...,wcâˆ’1â€‹,wc+1â€‹,wc+mâ€‹)=âˆ’logP(ucâ€‹âˆ£v^)=âˆ’logâˆ‘j=1âˆ£Vâˆ£â€‹exp(ujTâ€‹v^)exp(ucTâ€‹v^)â€‹=âˆ’ucTâ€‹v^+logj=1âˆ‘âˆ£Vâˆ£â€‹exp(ujTâ€‹v^)â€‹ . Skip-Gram . ä¸­é—´è¯é¢„æµ‹ä¸Šä¸‹æ–‡ä¸­çš„è¯ã€‚ . . Notations . $w_i$: è¯è¡¨ $V$ ä¸­ç¬¬ $i$ ä¸ªå•è¯ã€‚ | $ mathcal{V} in R^{n times lvert V rvert}$: è¾“å…¥å•è¯çŸ©é˜µã€‚ | $ mathcal{v_i}$: $ mathcal{V}$ çš„ç¬¬ $i$ åˆ—ï¼Œ$w_i$ çš„è¾“å…¥å‘é‡è¡¨ç¤ºã€‚ | $ mathcal{U} in R^{ lvert V rvert times n}$: è¾“å‡ºå•è¯çŸ©é˜µã€‚ | $ mathcal{u_i}$: $ mathcal{U}$ çš„ç¬¬ $i$ è¡Œï¼Œ$w_i$ çš„è¾“å‡ºå‘é‡è¡¨ç¤ºã€‚ | . Steps . ä¸­å¿ƒè¯ç”¨ç‹¬çƒ­ç¼–ç è¡¨ç¤º: $x in{R^{ lvert V rvert}}$ | è¾“å…¥å‘é‡è½¬æ¢: $v_c= mathcal{V}x in{R^n}$ | å¾—åˆ†: $z= mathcal{U}v_c in{R^{ lvert V rvert}}$ | æ¦‚ç‡: $ hat{y}=softmax(z) in{R^{ lvert V rvert}}$ï¼Œ$ hat{y}^{c-m},â€¦, hat{y}^{c-1}, hat{y}^{c+1},â€¦, hat{y}^{c+m}$ æ˜¯ä¸ä¸Šä¸‹æ–‡ä¸­è¯å¯¹åº”çš„æ¦‚ç‡ | æ¯”è¾ƒ: ç‹¬çƒ­ç¼–ç è¡¨ç¤º $y^{c-m},â€¦,y^{c-1},y^{c+1},â€¦,y^{c+m}$ å’Œå¯¹åº”çš„æ¦‚ç‡ | Object Function . minimizeÂ J=âˆ’logâ¡P(wcâˆ’m,...,wcâˆ’1,wc+1,wc+mâˆ£wc)=âˆ’logâ¡âˆj=0,j=Ì¸m2mP(wcâˆ’m+jâˆ£wc)=âˆ’logâ¡âˆj=0,j=Ì¸m2mP(ucâˆ’m+jâˆ£vc)=âˆ’logâ¡âˆj=0,j=Ì¸m2mexp(ucâˆ’m+jTvc)âˆ‘k=1âˆ£Vâˆ£exp(ukTvc)=âˆ’âˆ‘j=0,j=Ì¸mucâˆ’m+jTvc+2mlogâ¡âˆ‘k=1âˆ£Vâˆ£exp(ukTvc) begin{aligned} minimize J &amp;= - log{P(w_{c-m},...,w_{c-1},w_{c+1},w_{c+m}|w_c)} &amp;= - log{ prod_{j=0,j not = m}^{2m}P(w_{c-m+j}|w_c)} &amp;= - log{ prod_{j=0,j not = m}^{2m}P(u_{c-m+j}|v_c)} &amp;= - log{ prod_{j=0,j not = m}^{2m} frac{exp(u_{c-m+j}^Tv_c)}{ sum_{k=1}^{ lvert V rvert}exp(u_k^Tv_c)}} &amp;= - sum_{j=0,j not = m}u_{c-m+j}^Tv_c + 2m log{ sum_{k=1}^{ lvert V rvert}exp(u_k^Tv_c)} end{aligned}minimizeÂ Jâ€‹=âˆ’logP(wcâˆ’mâ€‹,...,wcâˆ’1â€‹,wc+1â€‹,wc+mâ€‹âˆ£wcâ€‹)=âˆ’logj=0,jî€ â€‹=mâˆ2mâ€‹P(wcâˆ’m+jâ€‹âˆ£wcâ€‹)=âˆ’logj=0,jî€ â€‹=mâˆ2mâ€‹P(ucâˆ’m+jâ€‹âˆ£vcâ€‹)=âˆ’logj=0,jî€ â€‹=mâˆ2mâ€‹âˆ‘k=1âˆ£Vâˆ£â€‹exp(ukTâ€‹vcâ€‹)exp(ucâˆ’m+jTâ€‹vcâ€‹)â€‹=âˆ’j=0,jî€ â€‹=mâˆ‘â€‹ucâˆ’m+jTâ€‹vcâ€‹+2mlogk=1âˆ‘âˆ£Vâˆ£â€‹exp(ukTâ€‹vcâ€‹)â€‹ . Negative Sampling . ä»¥ä¸Šç›®æ ‡å‡½æ•°çš„æ¯ä¸€æ¬¡æ›´æ–°éƒ½è¦éå†æ•´ä¸ªè¯è¡¨ï¼Œå½“è¯è¡¨å¾ˆå¤§çš„æ—¶å€™ï¼Œè®¡ç®—é‡å°±éå¸¸å¤§ï¼Œæ‰€ä»¥å¯ä»¥è´Ÿé‡‡æ ·å°‘è®¸æ ·æœ¬ï¼Œé™ä½è®¡ç®—é‡ã€‚ . æ­¤æ—¶çš„ç›®æ ‡å‡½æ•°ä¸ä¸€æ ·äº†ã€‚è€ƒè™‘ä¸€ç»„å•è¯ $(w,c)$ ,åˆ†åˆ«ä¸ºæ˜¯ä¸­å¿ƒè¯å’Œä¸Šä¸‹æ–‡ä¸­çš„è¯ï¼Œç”¨ $P(D=1 lvert w,c)$ è¡¨ç¤º $(w,c)$ æ¥è‡ªäºè¯­æ–™åº“çš„æ¦‚ç‡ï¼Œè€Œ $P(D=0 lvert w,c)$ åˆ™è¡¨ç¤º $(w,c)$ æ¥è‡ªéè¯­æ–™åº“çš„æ¦‚ç‡ï¼Œç›®æ ‡å‡½æ•°å°±æ˜¯è®©è¿™ä¸¤è€…çš„æ¦‚ç‡éƒ½å°½å¯èƒ½çš„å¤§ã€‚ . Î¸=argmaxÎ¸âˆ(w,c)âˆˆDP(D=1âˆ£w,c,Î¸)âˆ(w,c)âˆˆD~P(D=0âˆ£w,c,Î¸)=argmaxÎ¸âˆ(w,c)âˆˆDP(D=1âˆ£w,c,Î¸)âˆ(w,c)âˆˆD~(1âˆ’P(D=0âˆ£w,c,Î¸))=argmaxÎ¸âˆ‘(w,c)âˆˆDlogâ¡P(D=1âˆ£w,c,Î¸)+âˆ‘(w,c)âˆˆD~logâ¡(1âˆ’P(D=0âˆ£w,c,Î¸))=argmaxÎ¸âˆ‘(w,c)âˆˆDlogâ¡11+exp(âˆ’uwTvc)+âˆ‘(w,c)âˆˆD~logâ¡(1âˆ’11+exp(âˆ’uwTvc))=argmaxÎ¸âˆ‘(w,c)âˆˆDlogâ¡11+exp(âˆ’uwTvc)+âˆ‘(w,c)âˆˆD~logâ¡11+exp(uwTvc) begin{aligned} . theta &amp;= underset{ theta}{argmax} prod_{(w,c) in D}P(D=1|w,c, theta) prod_{(w,c) in tilde{D}}P(D=0|w,c, theta) &amp;= underset{ theta}{argmax} prod_{(w,c) in D}P(D=1|w,c, theta) prod_{(w,c) in tilde{D}}(1-P(D=0|w,c, theta)) &amp;= underset{ theta}{argmax} sum_{(w,c) in D} log P(D=1|w,c, theta) + sum_{(w,c) in tilde{D}} log (1-P(D=0|w,c, theta)) &amp;= underset{ theta}{argmax} sum_{(w,c) in D} log frac{1}{1+exp(-u_w^Tv_c)} + sum_{(w,c) in tilde{D}} log (1- frac{1}{1+exp(-u_w^Tv_c)}) &amp;= underset{ theta}{argmax} sum_{(w,c) in D} log frac{1}{1+exp(-u_w^Tv_c)} + sum_{(w,c) in tilde{D}} log frac{1}{1+exp(u_w^Tv_c)} end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;Î¸â€‹=Î¸argmaxâ€‹(w,c)âˆˆDâˆâ€‹P(D=1âˆ£w,c,Î¸)(w,c)âˆˆD~âˆâ€‹P(D=0âˆ£w,c,Î¸)=Î¸argmaxâ€‹(w,c)âˆˆDâˆâ€‹P(D=1âˆ£w,c,Î¸)(w,c)âˆˆD~âˆâ€‹(1âˆ’P(D=0âˆ£w,c,Î¸))=Î¸argmaxâ€‹(w,c)âˆˆDâˆ‘â€‹logP(D=1âˆ£w,c,Î¸)+(w,c)âˆˆD~âˆ‘â€‹log(1âˆ’P(D=0âˆ£w,c,Î¸))=Î¸argmaxâ€‹(w,c)âˆˆDâˆ‘â€‹log1+exp(âˆ’uwTâ€‹vcâ€‹)1â€‹+(w,c)âˆˆD~âˆ‘â€‹log(1âˆ’1+exp(âˆ’uwTâ€‹vcâ€‹)1â€‹)=Î¸argmaxâ€‹(w,c)âˆˆDâˆ‘â€‹log1+exp(âˆ’uwTâ€‹vcâ€‹)1â€‹+(w,c)âˆˆD~âˆ‘â€‹log1+exp(uwTâ€‹vcâ€‹)1â€‹â€‹&lt;/span&gt;&lt;/span&gt; . åˆ™ï¼š . J=âˆ’âˆ‘(w,c)âˆˆDlogâ¡11+exp(âˆ’uwTvc)âˆ’âˆ‘(w,c)âˆˆD~logâ¡11+exp(uwTvc)J = - sum_{(w,c) in D} log frac{1}{1+exp(-u_w^Tv_c)} - sum_{(w,c) in tilde{D}} log frac{1}{1+exp(u_w^Tv_c)}J=âˆ’(w,c)âˆˆDâˆ‘â€‹log1+exp(âˆ’uwTâ€‹vcâ€‹)1â€‹âˆ’(w,c)âˆˆD~âˆ‘â€‹log1+exp(uwTâ€‹vcâ€‹)1â€‹ . åœ¨ skip-gram ä¸­ï¼Œç»™å®šä¸­å¿ƒè¯ $c$ï¼Œå¯¹äºä¸Šä¸‹æ–‡çš„è¯ $c-m+j$ çš„ç›®æ ‡å‡½æ•°æ˜¯ï¼š . âˆ’logâ¡Ïƒ(ucâˆ’m+jTvc)âˆ’âˆ‘k=1Klogâ¡Ïƒ(âˆ’u~kTvc)- log sigma(u_{c-m+j}^T v_c) - sum_{k=1}^K log sigma(- tilde{u}_k^T v_c)âˆ’logÏƒ(ucâˆ’m+jTâ€‹vcâ€‹)âˆ’k=1âˆ‘Kâ€‹logÏƒ(âˆ’u~kTâ€‹vcâ€‹) . åœ¨ cbow ä¸­ï¼Œç»™å®šä¸Šä¸‹æ–‡å‘é‡ $ hat{v}= frac{v_{c-m}+â€¦+v_{c-1}+v_{c+1}+â€¦+v_{c+m}}{2m}$ï¼Œå¯¹äºä¸­å¿ƒè¯ $c$ çš„ç›®æ ‡å‡½æ•°æ˜¯ï¼š . âˆ’logâ¡Ïƒ(ucTv^)âˆ’âˆ‘k=1Klogâ¡Ïƒ(âˆ’u~kTv^)- log sigma(u_c^T hat{v}) - sum_{k=1}^K log sigma(- tilde{u}_k^T hat{v})âˆ’logÏƒ(ucTâ€‹v^)âˆ’k=1âˆ‘Kâ€‹logÏƒ(âˆ’u~kTâ€‹v^) . ä»¥ä¸Šå…¬å¼ä¸­ï¼Œ${ hat{u_k} lvert k=1,2,..,K}$ ä¾æ¦‚ç‡ $P_n(w)$ é‡‡æ ·ï¼Œä¸€èˆ¬ä¸ºä¸€å…ƒæ¨¡å‹çš„ $ frac{3}{4}$ æ¬¡æ–¹ï¼š . Pn(w)=Punigram(w)34P_n(w)={P_{unigram}(w)}^{ frac{3}{4}}Pnâ€‹(w)=Punigramâ€‹(w)43â€‹ . Hierarchical Softmax . Hierarchical softmax åˆ©ç”¨äºŒå‰æ ‘æ¥è¡¨ç¤ºè¯è¡¨å’Œè®¡ç®—æ¦‚ç‡ã€‚æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªè¯ï¼Œä»æ ¹ç»“ç‚¹åˆ°å¶å­èŠ‚ç‚¹æœ‰å”¯ä¸€çš„ä¸€æ¡è·¯å¾„ã€‚æ²¡æœ‰è¾“å‡ºå‘é‡ï¼Œè€Œé™¤äº†æ ¹ç»“ç‚¹å’Œå¶å­èŠ‚ç‚¹ï¼Œæ ‘ä¸­çš„èŠ‚ç‚¹éƒ½å¯¹åº”ä¸€ä¸ªéœ€è¦å­¦ä¹ çš„å‘é‡ã€‚è¾“å‡ºæŸä¸€å¶å­èŠ‚ç‚¹çš„æ¦‚ç‡ä¸º ä»æ ¹ç»“ç‚¹åˆ°æ­¤å¶å­èŠ‚ç‚¹è·¯å¾„å‡ºç°çš„æ¦‚ç‡ï¼Œæ‰€ä»¥å¤æ‚åº¦ç”± O($ vert$V$ vert$) å˜ä¸º O(log$ vert$V$ vert$)ã€‚ . . Notations . $L(w)$: ä»æ ¹ç»“ç‚¹åˆ°å¶å­èŠ‚ç‚¹ $w$ çš„èŠ‚ç‚¹æ•° | $n(w,i)$: è·¯å¾„ä¸­ç¬¬ $i$ ä¸ªèŠ‚ç‚¹ï¼Œæ‰€ä»¥ $n(w,1)$ æ˜¯æ ¹ç»“ç‚¹ï¼Œ$n(w,L(w))$ æ˜¯å¶å­èŠ‚ç‚¹ $w$ çš„çˆ¶èŠ‚ç‚¹ | $v_{n(w,i)}$: è·¯å¾„ä¸­ç¬¬ $i$ ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å‘é‡ | $ch(n)$: å¯¹ä¸å†…éƒ¨èŠ‚ç‚¹ $n$ï¼Œæ¯æ¬¡éƒ½é€‰æ‹©å…¶å·¦å­©å­ï¼ˆæˆ–è€…å³å­©å­ï¼‰ | $w_i$: è¾“å…¥å•è¯ | . Object Function . å¯¹äºè¾“å…¥å•è¯ $w_i$ï¼Œè¾“å‡ºå•è¯ä¸º $w$ çš„æ¦‚ç‡ä¸ºï¼š . P(wâˆ£wi)=âˆj=1L(w)âˆ’1Ïƒ([n(w,j+1)=ch(n(w,j))]â‹…vn(w,j)Tvwi)P(w vert w_i) = prod_{j=1}^{L(w)-1} sigma([n(w,j+1)=ch(n(w,j))] cdot v_{n(w,j)}^T v_{w_i})P(wâˆ£wiâ€‹)=j=1âˆL(w)âˆ’1â€‹Ïƒ([n(w,j+1)=ch(n(w,j))]â‹…vn(w,j)Tâ€‹vwiâ€‹â€‹) . å¹¶ä¸” . [x]={1ifÂ xÂ isÂ trueâˆ’1otherwise[x] = begin{cases} 1 &amp; text{if x is true} -1 &amp; text{otherwise} end{cases}[x]={1âˆ’1â€‹ifÂ xÂ isÂ trueotherwiseâ€‹ . ä¸Šå¼ä¿è¯äº†åœ¨èŠ‚ç‚¹ $n$ å¤„ï¼Œæœ‰ï¼š . Ïƒ(vnTvwi)+Ïƒ(âˆ’vnTvwi)=1 sigma(v_n^T v_{w_i}) + sigma(-v_n^T v_{w_i}) = 1Ïƒ(vnTâ€‹vwiâ€‹â€‹)+Ïƒ(âˆ’vnTâ€‹vwiâ€‹â€‹)=1 . è€Œä¸”åƒåŸå§‹çš„ softmax ä¸€æ ·ï¼Œæœ‰ï¼š . âˆ‘w=1âˆ£Vâˆ£P(wâˆ£wi)=1 sum_{w=1}^{ lvert V rvert} P(w vert w_i) = 1w=1âˆ‘âˆ£Vâˆ£â€‹P(wâˆ£wiâ€‹)=1 . åœ¨ cbow ä¸­ï¼Œ$v_{w_i}= frac{v_{c-m}+â€¦+v_{c-1}+v_{c+1}+â€¦+v_{c+m}}{2m}$ï¼›åœ¨ skip-gram ä¸­ï¼Œ$v_{w_i}=v_c$ï¼Œå³ä¸­å¿ƒè¯çš„è¾“å…¥å‘é‡ã€‚ç›®æ ‡å‡½æ•°ä¸ºï¼š . minimizeâ€…â€ŠJ=âˆ’logâ¡P(wâˆ£wi)=âˆj=1L(w)âˆ’1Ïƒ([n(w,j+1)=ch(n(w,j))]â‹…vn(w,j)Tvwi) begin{aligned} minimize thickspace J &amp;= - log P(w vert w_i) &amp;= prod_{j=1}^{L(w)-1} sigma([n(w,j+1)=ch(n(w,j))] cdot v_{n(w,j)}^T v_{w_i}) end{aligned}minimizeJâ€‹=âˆ’logP(wâˆ£wiâ€‹)=j=1âˆL(w)âˆ’1â€‹Ïƒ([n(w,j+1)=ch(n(w,j))]â‹…vn(w,j)Tâ€‹vwiâ€‹â€‹)â€‹ . è®­ç»ƒæ—¶ï¼Œåªç”¨æ›´æ–°ä»æ ¹ç»“ç‚¹åˆ°å¯¹åº”å¶å­èŠ‚ç‚¹è·¯å¾„ä¸Šçš„ç‚¹çš„å‘é‡å³å¯ã€‚äºŒå‰æ ‘ä¸€èˆ¬ç”¨ Huffman æ ‘æ„å»ºï¼Œé¢‘æ¬¡é«˜çš„è¯ç”¨æœ‰è¾ƒå°çš„è·¯å¾„é•¿åº¦ï¼Œå¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚åœ¨å®é™…ä¸­ï¼Œhierarchical softmax å¯¹äºé¢‘æ¬¡è¾ƒä½çš„è¯æ¯”è¾ƒå‹å¥½ï¼Œå› ä¸ºä¸æ¶‰åŠä¾æ®é¢‘æ¬¡è´Ÿé‡‡æ ·çš„é—®é¢˜ï¼›negative sampling å¯¹äºé¢‘æ¬¡è¾ƒé«˜çš„è¯å’Œä½ç»´åº¦å‘é‡æ¯”è¾ƒå‹å¥½ã€‚ . SGNS (Skip-Gram with Negative Sampling) . åœ¨è¿™ä¸€èŠ‚ï¼Œä½œä¸ºç»ƒä¹ ï¼Œæˆ‘ä»¬è¦å®ç° SGNS ã€‚ . Naive Softmax Loss . VâˆˆRVÃ—dUâˆˆRVÃ—dV in R^{V times d} U in R^{V times d}VâˆˆRVÃ—dUâˆˆRVÃ—d . P(O=oâˆ£C=c)=exp(uoTvc)âˆ‘wâˆˆVexp(uwTvc)P(O=o vert C=c) = frac{exp(u_o^T v_c)}{ sum_{w in V} exp(u_w^T v_c)}P(O=oâˆ£C=c)=âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)exp(uoTâ€‹vcâ€‹)â€‹ . Jnaiveâˆ’softmax(vc,o,U)=âˆ’âˆ‘wâˆˆVywlogâ¡(y^w)=âˆ’1Ã—logâ¡P(O=oâˆ£C=c)=âˆ’logâ¡P(O=oâˆ£C=c)=âˆ’logâ¡exp(uoTvc)âˆ‘wâˆˆVexp(uwTvc)=âˆ’logâ¡(y^o) begin{aligned} J_{naive-softmax(v_c,o,U)} &amp;= - sum_{w in V} y_w log( hat{y}_w) &amp;= - 1 times log P(O=o vert C=c) &amp;= - log P(O=o vert C=c) &amp;= - log frac{exp(u_o^T v_c)}{ sum_{w in V} exp(u_w^T v_c)} &amp;= - log ( hat{y}_o) end{aligned}Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹=âˆ’wâˆˆVâˆ‘â€‹ywâ€‹log(y^â€‹wâ€‹)=âˆ’1Ã—logP(O=oâˆ£C=c)=âˆ’logP(O=oâˆ£C=c)=âˆ’logâˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)exp(uoTâ€‹vcâ€‹)â€‹=âˆ’log(y^â€‹oâ€‹)â€‹ . ç›®æ ‡å‡½æ•°å¯¹ $v_c$ çš„åå¯¼ï¼š . âˆ‚Jnaiveâˆ’softmax(vc,o,U)âˆ‚vc=âˆ’âˆ‚(uoTvc)âˆ‚vc+âˆ‚(logâ¡âˆ‘wâˆˆVexp(uwTvc))âˆ‚vc=âˆ’uo+1âˆ‘wâˆˆVexp(uwTvc)âˆ‚(âˆ‘wâˆˆVexp(uwTvc))âˆ‚vc=âˆ’uo+âˆ‘wâˆˆVexp(uwTvc)uwâˆ‘wâˆˆVexp(uwTvc)=âˆ’uo+âˆ‘wâˆˆVP(O=wâˆ£C=c)uw=âˆ’uo+âˆ‘wâˆˆVy^wuw=âˆ’UTy+UTy^=UT(y^âˆ’y) begin{aligned} frac{ partial J_{naive-softmax(v_c,o,U)}}{ partial v_c} &amp;= - frac{ partial (u_o^T v_c)}{ partial v_c} + frac{ partial ( log sum_{w in V} exp(u_w^T v_c))}{ partial v_c} &amp;= -u_o + frac{1}{ sum_{w in V} exp(u_w^T v_c)} frac{ partial ( sum_{w in V} exp(u_w^T v_c))}{ partial v_c} &amp;= -u_o + sum_{w in V} frac{exp(u_w^T v_c) u_w}{ sum_{w in V} exp(u_w^T v_c)} &amp;= -u_o + sum_{w in V} P(O=w vert C=c) u_w &amp;= -u_o + sum_{w in V} hat{y}_w u_w &amp;= -U^Ty + U^T hat{y} &amp;= U^T( hat{y}-y) end{aligned}âˆ‚vcâ€‹âˆ‚Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’âˆ‚vcâ€‹âˆ‚(uoTâ€‹vcâ€‹)â€‹+âˆ‚vcâ€‹âˆ‚(logâˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ’uoâ€‹+âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)1â€‹âˆ‚vcâ€‹âˆ‚(âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ’uoâ€‹+wâˆˆVâˆ‘â€‹âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)exp(uwTâ€‹vcâ€‹)uwâ€‹â€‹=âˆ’uoâ€‹+wâˆˆVâˆ‘â€‹P(O=wâˆ£C=c)uwâ€‹=âˆ’uoâ€‹+wâˆˆVâˆ‘â€‹y^â€‹wâ€‹uwâ€‹=âˆ’UTy+UTy^â€‹=UT(y^â€‹âˆ’y)â€‹ . ç›®æ ‡å‡½æ•°å¯¹ $u_w$ çš„åå¯¼ï¼š . âˆ‚Jnaiveâˆ’softmax(vc,o,U)âˆ‚uw=âˆ’âˆ‚(uoTvc)âˆ‚uw+âˆ‚(logâ¡âˆ‘wâˆˆVexp(uwTvc))âˆ‚uw begin{aligned} frac{ partial J_{naive-softmax(v_c,o,U)}}{ partial u_w} &amp;= - frac{ partial (u_o^T v_c)}{ partial u_w} + frac{ partial ( log sum_{w in V} exp(u_w^T v_c))}{ partial u_w} end{aligned}âˆ‚uwâ€‹âˆ‚Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’âˆ‚uwâ€‹âˆ‚(uoTâ€‹vcâ€‹)â€‹+âˆ‚uwâ€‹âˆ‚(logâˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹â€‹ . å½“ $w not = o$ æ—¶ï¼š . âˆ‚Jnaiveâˆ’softmax(vc,o,U)âˆ‚uw=âˆ’0+âˆ‚(logâ¡âˆ‘wâˆˆVexp(uwTvc))âˆ‚uw=1âˆ‘wâˆˆVexp(uwTvc)âˆ‚(âˆ‘wâˆˆVexp(uwTvc))âˆ‚uw=exp(uwTvc)âˆ‘wâˆˆVexp(uwTvc)vc=y^wvc=(y^wâˆ’0)vc begin{aligned} frac{ partial J_{naive-softmax(v_c,o,U)}}{ partial u_w} &amp;= -0 + frac{ partial ( log sum_{w in V} exp(u_w^T v_c))}{ partial u_w} &amp;= frac{1}{ sum_{w in V} exp(u_w^T v_c)} frac{ partial ( sum_{w in V} exp(u_w^T v_c))}{ partial u_w} &amp;= frac{exp(u_w^T v_c)}{ sum_{w in V} exp(u_w^T v_c)} v_c &amp;= hat{y}_w v_c &amp;= ( hat{y}_w-0)v_c end{aligned}âˆ‚uwâ€‹âˆ‚Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’0+âˆ‚uwâ€‹âˆ‚(logâˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)1â€‹âˆ‚uwâ€‹âˆ‚(âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)exp(uwTâ€‹vcâ€‹)â€‹vcâ€‹=y^â€‹wâ€‹vcâ€‹=(y^â€‹wâ€‹âˆ’0)vcâ€‹â€‹ . å½“ $w = o$ æ—¶ï¼š . âˆ‚Jnaiveâˆ’softmax(vc,o,U)âˆ‚uw=âˆ’vc+âˆ‚(logâ¡âˆ‘wâˆˆVexp(uwTvc))âˆ‚uw=âˆ’vc+1âˆ‘wâˆˆVexp(uwTvc)âˆ‚(âˆ‘wâˆˆVexp(uwTvc))âˆ‚uw=âˆ’vc+exp(uwTvc)âˆ‘wâˆˆVexp(uwTvc)vc=âˆ’vc+y^wvc=(y^wâˆ’1)vc begin{aligned} frac{ partial J_{naive-softmax(v_c,o,U)}}{ partial u_w} &amp;= -v_c + frac{ partial ( log sum_{w in V} exp(u_w^T v_c))}{ partial u_w} &amp;= -v_c + frac{1}{ sum_{w in V} exp(u_w^T v_c)} frac{ partial ( sum_{w in V} exp(u_w^T v_c))}{ partial u_w} &amp;= -v_c + frac{exp(u_w^T v_c)}{ sum_{w in V} exp(u_w^T v_c)} v_c &amp;= -v_c + hat{y}_w v_c &amp;= ( hat{y}_w-1)v_c end{aligned}âˆ‚uwâ€‹âˆ‚Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’vcâ€‹+âˆ‚uwâ€‹âˆ‚(logâˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ’vcâ€‹+âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)1â€‹âˆ‚uwâ€‹âˆ‚(âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹))â€‹=âˆ’vcâ€‹+âˆ‘wâˆˆVâ€‹exp(uwTâ€‹vcâ€‹)exp(uwTâ€‹vcâ€‹)â€‹vcâ€‹=âˆ’vcâ€‹+y^â€‹wâ€‹vcâ€‹=(y^â€‹wâ€‹âˆ’1)vcâ€‹â€‹ . æ‰€ä»¥ï¼š . âˆ‚Jnaiveâˆ’softmax(vc,o,U)âˆ‚U=(y^âˆ’y)vcT begin{aligned} frac{ partial J_{naive-softmax(v_c,o,U)}}{ partial U} = ( hat{y}-y)v_c^T end{aligned}âˆ‚Uâˆ‚Jnaiveâˆ’softmax(vcâ€‹,o,U)â€‹â€‹=(y^â€‹âˆ’y)vcTâ€‹â€‹ . Sigmoid . Ïƒ(x)=11+eâˆ’x=ex1+ex begin{aligned} sigma(x) &amp;= frac{1}{1+e^{-x}} &amp;= frac{e^x}{1+e^x} end{aligned}Ïƒ(x)â€‹=1+eâˆ’x1â€‹=1+exexâ€‹â€‹ . âˆ‚Ïƒ(xi)âˆ‚xi=exi(1+exi)âˆ’e2xi(1+exi)2=exi1+exi11+exi=Ïƒ(xi)(1âˆ’Ïƒ(xi)) begin{aligned} frac{ partial sigma(x_i)}{ partial x_i} &amp;= frac{e^{x_i}(1+e^{x_i})-e^{2x_i}}{(1+e^{x_i})^2} &amp;= frac{e^{x_i}}{1+e^{x_i}} frac{1}{1+e^{x_i}} &amp;= sigma(x_i)(1- sigma(x_i)) end{aligned}âˆ‚xiâ€‹âˆ‚Ïƒ(xiâ€‹)â€‹â€‹=(1+exiâ€‹)2exiâ€‹(1+exiâ€‹)âˆ’e2xiâ€‹â€‹=1+exiâ€‹exiâ€‹â€‹1+exiâ€‹1â€‹=Ïƒ(xiâ€‹)(1âˆ’Ïƒ(xiâ€‹))â€‹ . âˆ‚Ïƒ(x)âˆ‚x=[âˆ‚Ïƒ(xi)âˆ‚xi]dÃ—d=[Ïƒâ€²(x1)0...00Ïƒâ€²(x2)...0â‹®â‹®â‹®â‹®000Ïƒâ€²(xd)]=diag(Ïƒâ€²(x)) begin{aligned} frac{ partial sigma(x)}{ partial x} &amp;= left[ frac{ partial sigma(x_i)}{ partial x_i} right]_{d times d} &amp;= begin{bmatrix} sigma&amp;#x27;(x_1) &amp; 0 &amp; ... &amp; 0 0 &amp; sigma&amp;#x27;(x_2) &amp; ... &amp; 0 vdots &amp; vdots &amp; vdots &amp; vdots 0 &amp; 0 &amp; 0 &amp; sigma&amp;#x27;(x_d) end{bmatrix} &amp;= text{diag}( sigma^ prime(x)) . end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;âˆ‚xâˆ‚Ïƒ(x)â€‹â€‹=[âˆ‚xiâ€‹âˆ‚Ïƒ(xiâ€‹)â€‹]dÃ—dâ€‹=â£â¢â¢â¢â¢â¡â€‹Ïƒâ€²(x1â€‹)0â‹®0â€‹0Ïƒâ€²(x2â€‹)â‹®0â€‹......â‹®0â€‹00â‹®Ïƒâ€²(xdâ€‹)â€‹â¦â¥â¥â¥â¥â¤â€‹=diag(Ïƒâ€²(x))â€‹&lt;/span&gt;&lt;/span&gt; . Negative Sampling Loss . Jnegâˆ’sample(vc,o,U)=âˆ’logâ¡(Ïƒ(uoTvc))âˆ’âˆ‘k=1Klogâ¡(Ïƒ(âˆ’ukTvc))J_{neg-sample(v_c,o,U)} = - log( sigma(u_o^T v_c)) - sum_{k=1}^K log( sigma(-u_k^T v_c))Jnegâˆ’sample(vcâ€‹,o,U)â€‹=âˆ’log(Ïƒ(uoTâ€‹vcâ€‹))âˆ’k=1âˆ‘Kâ€‹log(Ïƒ(âˆ’ukTâ€‹vcâ€‹)) . å¯¹ $v_c$ çš„åå¯¼ï¼š . âˆ‚Jnegâˆ’sample(vc,o,U)âˆ‚vc=âˆ’1Ïƒ(uoTvc)Ïƒ(uoTvc)(1âˆ’Ïƒ(uoTvc))uo+âˆ‘k=1K1Ïƒ(âˆ’ukTvc)Ïƒ(âˆ’ukTvc)(1âˆ’Ïƒ(âˆ’ukTvc))uk=(Ïƒ(uoTvc)âˆ’1)uo+âˆ‘k=1K(1âˆ’Ïƒ(âˆ’ukTvc))uk=(Ïƒ(uoTvc)âˆ’1)uo+âˆ‘k=1KÏƒ(ukTvc)uk=âˆ’Ïƒ(âˆ’uoTvc)uo+âˆ‘k=1KÏƒ(ukTvc)uk begin{aligned} frac{ partial J_{neg-sample(v_c,o,U)}}{ partial v_c} &amp;= - frac{1}{ sigma(u_o^T v_c)} sigma(u_o^T v_c)(1- sigma(u_o^T v_c))u_o + sum_{k=1}^K frac{1}{ sigma(-u_k^T v_c)} sigma(-u_k^T v_c)(1- sigma(-u_k^T v_c))u_k &amp;= ( sigma(u_o^T v_c)-1)u_o + sum_{k=1}^K (1- sigma(-u_k^T v_c))u_k &amp;= ( sigma(u_o^T v_c)-1)u_o + sum_{k=1}^K sigma(u_k^T v_c)u_k &amp;= - sigma(-u_o^T v_c)u_o + sum_{k=1}^K sigma(u_k^T v_c)u_k end{aligned}âˆ‚vcâ€‹âˆ‚Jnegâˆ’sample(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’Ïƒ(uoTâ€‹vcâ€‹)1â€‹Ïƒ(uoTâ€‹vcâ€‹)(1âˆ’Ïƒ(uoTâ€‹vcâ€‹))uoâ€‹+k=1âˆ‘Kâ€‹Ïƒ(âˆ’ukTâ€‹vcâ€‹)1â€‹Ïƒ(âˆ’ukTâ€‹vcâ€‹)(1âˆ’Ïƒ(âˆ’ukTâ€‹vcâ€‹))ukâ€‹=(Ïƒ(uoTâ€‹vcâ€‹)âˆ’1)uoâ€‹+k=1âˆ‘Kâ€‹(1âˆ’Ïƒ(âˆ’ukTâ€‹vcâ€‹))ukâ€‹=(Ïƒ(uoTâ€‹vcâ€‹)âˆ’1)uoâ€‹+k=1âˆ‘Kâ€‹Ïƒ(ukTâ€‹vcâ€‹)ukâ€‹=âˆ’Ïƒ(âˆ’uoTâ€‹vcâ€‹)uoâ€‹+k=1âˆ‘Kâ€‹Ïƒ(ukTâ€‹vcâ€‹)ukâ€‹â€‹ . å¯¹ $u_o$ çš„åå¯¼ï¼š . âˆ‚Jnegâˆ’sample(vc,o,U)âˆ‚uo=âˆ’1Ïƒ(uoTvc)Ïƒ(uoTvc)(1âˆ’Ïƒ(uoTvc))vc=(Ïƒ(uoTvc)âˆ’1)vc=âˆ’Ïƒ(âˆ’uoTvc)vc begin{aligned} frac{ partial J_{neg-sample(v_c,o,U)}}{ partial u_o} &amp;= - frac{1}{ sigma(u_o^T v_c)} sigma(u_o^T v_c)(1- sigma(u_o^T v_c))v_c &amp;= ( sigma(u_o^T v_c)-1)v_c &amp;= - sigma(-u_o^T v_c)v_c end{aligned}âˆ‚uoâ€‹âˆ‚Jnegâˆ’sample(vcâ€‹,o,U)â€‹â€‹â€‹=âˆ’Ïƒ(uoTâ€‹vcâ€‹)1â€‹Ïƒ(uoTâ€‹vcâ€‹)(1âˆ’Ïƒ(uoTâ€‹vcâ€‹))vcâ€‹=(Ïƒ(uoTâ€‹vcâ€‹)âˆ’1)vcâ€‹=âˆ’Ïƒ(âˆ’uoTâ€‹vcâ€‹)vcâ€‹â€‹ . å¯¹ $u_k$ çš„åå¯¼ï¼š . âˆ‚Jnegâˆ’sample(vc,o,U)âˆ‚uk=1Ïƒ(âˆ’ukTvc)Ïƒ(âˆ’ukTvc)(1âˆ’Ïƒ(âˆ’ukTvc))vc=Ïƒ(ukTvc)vc begin{aligned} frac{ partial J_{neg-sample(v_c,o,U)}}{ partial u_k} &amp;= frac{1}{ sigma(-u_k^T v_c)} sigma(-u_k^T v_c)(1- sigma(-u_k^T v_c))v_c &amp;= sigma(u_k^T v_c)v_c end{aligned}âˆ‚ukâ€‹âˆ‚Jnegâˆ’sample(vcâ€‹,o,U)â€‹â€‹â€‹=Ïƒ(âˆ’ukTâ€‹vcâ€‹)1â€‹Ïƒ(âˆ’ukTâ€‹vcâ€‹)(1âˆ’Ïƒ(âˆ’ukTâ€‹vcâ€‹))vcâ€‹=Ïƒ(ukTâ€‹vcâ€‹)vcâ€‹â€‹ . Skip-Gram Loss . Jskipâˆ’gram(vc,wtâˆ’m,...,wt+m,U)=âˆ‘jâˆˆ[âˆ’t,t],j=Ì¸0Jnegâˆ’sample(vc,wt+j,U) begin{aligned} J_{skip-gram(v_c,w_{t-m},...,w_{t+m},U)} = sum_{j in [-t,t],j not ={0}} J_{neg-sample(v_c,w_{t+j},U)} end{aligned}Jskipâˆ’gram(vcâ€‹,wtâˆ’mâ€‹,...,wt+mâ€‹,U)â€‹=jâˆˆ[âˆ’t,t],jî€ â€‹=0âˆ‘â€‹Jnegâˆ’sample(vcâ€‹,wt+jâ€‹,U)â€‹â€‹ . å¯¹ $U$ çš„åå¯¼ï¼š . âˆ‚Jskipâˆ’gram(vc,wtâˆ’m,...,wt+m,U)âˆ‚U=âˆ‘jâˆˆ[âˆ’t,t],j=Ì¸0âˆ‚Jnegâˆ’sample(vc,wt+j,U)âˆ‚U begin{aligned} frac{ partial J_{skip-gram(v_c,w_{t-m},...,w_{t+m},U)}}{ partial U} = sum_{j in [-t,t],j not ={0}} frac{ partial J_{neg-sample(v_c,w_{t+j},U)}}{ partial U} end{aligned}âˆ‚Uâˆ‚Jskipâˆ’gram(vcâ€‹,wtâˆ’mâ€‹,...,wt+mâ€‹,U)â€‹â€‹=jâˆˆ[âˆ’t,t],jî€ â€‹=0âˆ‘â€‹âˆ‚Uâˆ‚Jnegâˆ’sample(vcâ€‹,wt+jâ€‹,U)â€‹â€‹â€‹ . å¯¹ $v_c$ çš„åå¯¼ï¼š . âˆ‚Jskipâˆ’gram(vc,wtâˆ’m,...,wt+m,U)âˆ‚vc=âˆ‘jâˆˆ[âˆ’t,t],j=Ì¸0âˆ‚Jnegâˆ’sample(vc,wt+j,U)âˆ‚vc begin{aligned} frac{ partial J_{skip-gram(v_c,w_{t-m},...,w_{t+m},U)}}{ partial v_c} &amp;= sum_{j in [-t,t],j not ={0}} frac{ partial J_{neg-sample(v_c,w_{t+j},U)}}{ partial v_c} end{aligned}âˆ‚vcâ€‹âˆ‚Jskipâˆ’gram(vcâ€‹,wtâˆ’mâ€‹,...,wt+mâ€‹,U)â€‹â€‹â€‹=jâˆˆ[âˆ’t,t],jî€ â€‹=0âˆ‘â€‹âˆ‚vcâ€‹âˆ‚Jnegâˆ’sample(vcâ€‹,wt+jâ€‹,U)â€‹â€‹â€‹ . å¯¹ $v_w (w not ={c})$ çš„åå¯¼ï¼š . âˆ‚Jskipâˆ’gram(vc,wtâˆ’m,...,wt+m,U)âˆ‚vc=0 begin{aligned} frac{ partial J_{skip-gram(v_c,w_{t-m},...,w_{t+m},U)}}{ partial v_c} &amp;= 0 end{aligned}âˆ‚vcâ€‹âˆ‚Jskipâˆ’gram(vcâ€‹,wtâˆ’mâ€‹,...,wt+mâ€‹,U)â€‹â€‹â€‹=0â€‹ . Code . åŸºäº CS224N ä½œä¸š2ï¼Œè§ word2vec. .",
            "url": "https://cbqin.github.io/jiangnandiqing/markdown/2021/02/08/word2vec.html",
            "relUrl": "/markdown/2021/02/08/word2vec.html",
            "date": " â€¢ Feb 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Some Interesting Resources",
            "content": "Road To M$ . Courses That Interesting . Mathematics . Information Theory, Pattern Recognition, and Neural Networks | The Matrix Calculus You Need For Deep Learning | 18.06 Linear Algebra | Statistical Rethinking: A Bayesian Course | . NLP . CS 124: From Languages to Information, Spring 2020 | CS 224n: Natural Language Processing with Deep Learning, Winter 2019 | CS 11-747: Neural Networks for NLP, Spring 2020 | CS 11-737: Multilingual Natural Language Processing | CS 520: Knowledge Graphs, Spring 2020 | . ML . INF8953CE: Machine Learning, Fall 2020 | Statistical Machine Learning, Summer 2020 | Probabilistic Machine Learning, Summer 2020 | 10-708: Probabilistic Graphical Models, Spring 2020 | Introduction to Statistical Learning Series | . DL . DS-GA 1008: Deep Learning, Spring 2020 | CS294-158-SP20: Deep Unsupervised Learning, Spring 2020 | CS 330: Deep Multi-Task and Meta Learning, Fall 2019 | CS 224W: Machine Learning with Graphs, Fall 2019 | ESE 680: Graph Neural Networks, Fall 2020 | Deep Learning and Bayesian Methods | . RL . CS 285: Deep Reinforcement Learning, 2020 | . Books . Mathematics for Machine Learning | Foundations of Machine Learning | Information Theory, Inference, and Learning Algorithms | Probability Theory: The Logic of Science | Introduction to Linear Algebra | Deep Learning | Speech and Natural Language Processing | Natural Language Processing | Pattern Recognition and Machine Learning | The Elements of Statistical Learnin | An Introduction to Statistical Learning | ç™¾é¢æœºå™¨å­¦ä¹  | ç™¾é¢æ·±åº¦å­¦ä¹  | Reinforcement Learning: An Introduction | . Algorithms . Leetcode | Algorithms | CS 170: Efficient Algorithms and Intractable Problems, fall 2020 | .",
            "url": "https://cbqin.github.io/jiangnandiqing/markdown/2020/10/20/resources.html",
            "relUrl": "/markdown/2020/10/20/resources.html",
            "date": " â€¢ Oct 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://cbqin.github.io/jiangnandiqing/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://cbqin.github.io/jiangnandiqing/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "æ±Ÿå—æ•Œæƒ… .",
          "url": "https://cbqin.github.io/jiangnandiqing/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://cbqin.github.io/jiangnandiqing/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}